---
title: "Ridge Regression"
author: "Karan Gill"
date: "2023-07-03"
output: html_document
---
Experimental Design:

In the below experiment a synthetic high dimensional regression scenario is created where the number of features is very large (n <- 100 features). Out of these 100 features 20 features are the relevant ones. These relevant features have been given the name 'num_relevant_features'. The X matrix is created using the rnorm function and the num_relevant_features. The train and test data is formulated and a regularisation parameter Lambda is defined as lambda_seq <- 10^seq(0, 10, by = 0.5). Ridge Regression is used to get the appropriate estimate.
To obtain a more concrete estimate this trial is conducted multiple times and the errors are averaged over these multiple trials. The errors for each singular run is stored in a matrix and the averaged out errors are stored in a dataframe error_data.
```{r}
# Load required libraries
library(glmnet)
library(caret)
library(ggplot2)
library(reshape2)


# Set the seed for reproducibility
set.seed(123)

# Define the number of observations and relevant features
num_features_total <- 200
num_examples_total <- 100

num_trials <- 100
prop_relevant_features <- 0.5
prop_train_examples <- 0.7
ci_level <- 0.95

# Define a range of regularization parameters
lambda_seq <- 10^seq(-3, 8, by = 0.5)

# Create empty matrices to store the results
train_errors <- matrix(0, nrow = num_trials, ncol = length(lambda_seq))
test_errors <- matrix(0, nrow = num_trials, ncol = length(lambda_seq))

num_relevant_features <- prop_relevant_features * num_features_total
num_train_examples <- prop_train_examples * num_examples_total 


# Perform the trials
for (i in 1:num_trials) {
  # Generate the feature matrix
  X <- matrix(rnorm(num_features_total * num_examples_total), ncol = num_features_total)
  
  # Generate the true coefficients
  true_coefficients <- c(rep(2, num_relevant_features), rep(0, (num_features_total - num_relevant_features)))
  
  # Generate the target variable
  y <- X %*% true_coefficients + rnorm(num_examples_total)
  
  # Convert data to a dataframe
  data <- data.frame(y, X)
  
  # Split the data into training and testing sets
  train_index <- createDataPartition(data$y, p = prop_train_examples, list = FALSE)
  train_data <- data[train_index, ]
  test_data <- data[-train_index, ]
 
   # Create an empty matrix to store the coefficients
  coefficients <- matrix(0, nrow = length(lambda_seq), ncol = dim(train_data[, -1])[2])
  
  # Lists to store the errors for this trial
  trial_train_errors <- vector("numeric", length(lambda_seq))
  trial_test_errors <- vector("numeric", length(lambda_seq))
  bias_squared <- vector("numeric", length(lambda_seq))
  variance <- vector("numeric", length(lambda_seq))
  
  # Perform Ridge regression for each lambda value
  for (j in seq_along(lambda_seq)) {
    # Train the Ridge model
    ridge_model <- glmnet(x = as.matrix(train_data[, -1]), y = train_data$y, alpha = 0, lambda = lambda_seq[j])
    
    # Make predictions on the training set
    train_predictions <- predict(ridge_model, newx = as.matrix(train_data[, -1]))
    
    # Compute the training error
    trial_train_errors[j] <- mean((train_data$y - train_predictions)^2)
    
    # Make predictions on the test set
    test_predictions <- predict(ridge_model, newx = as.matrix(test_data[, -1]))
    
    # Compute the test error
    trial_test_errors[j] <- mean((test_data$y - test_predictions)^2)
    
    # Compute the bias squared
    bias_squared[j] <- mean((train_predictions - train_data$y)^2)
    
    # Compute the variance
    variance[j] <- mean((train_predictions - mean(train_predictions))^2)
    
    # Store the coefficients for this lambda value
    coefficients[j, ] <- coef(ridge_model)[-1]
  }
  
  
  # Store the errors for this trial in the matrices
  train_errors[i, ] <- trial_train_errors
  test_errors[i, ] <- trial_test_errors
}

# Calculate the mean and standard deviation of the train and test errors
mean_train_errors <- apply(train_errors, 2, mean)
mean_test_errors <- apply(test_errors, 2, mean)
sd_train_errors <- apply(train_errors, 2, sd)
sd_test_errors <- apply(test_errors, 2, sd)

# Create a data frame with lambda and errors
error_data <- data.frame(lambda = lambda_seq, TrainError = mean_train_errors, TestError = mean_test_errors, TrainSE = sd_train_errors, TestSE = sd_test_errors)

```

A dataset called error_data is created which contains all the error values for the train and test data along with their corressponding lambda value. These error values have been used to calculate the confidence bands. Using these values a graph has been plotted where X axis is the regularisation parameter and Y axis is the train and test error.

```{r}
# Plot the train and test errors with confidence bands
ggplot(error_data,aes(x = log10(lambda))) +
  geom_line(aes(y = TrainError, color = "Train Error")) +
  geom_ribbon(aes(ymin = TrainError - qnorm(0.5*(1 + ci_level)) * TrainSE/sqrt(num_trials - 1), ymax = TrainError + qnorm(0.5*(1 + ci_level)) * TrainSE/sqrt(num_trials - 1), fill = "Train Error"), alpha = 0.2) +
  geom_line(aes(y = TestError, color = "Test Error")) +
  geom_ribbon(aes(ymin = TestError - qnorm(0.5*(1 + ci_level)) * TestSE/sqrt(num_trials - 1), ymax = TestError + qnorm(0.5*(1 + ci_level)) * TestSE/sqrt(num_trials - 1), fill = "Test Error"), alpha = 0.2) +
  scale_color_manual(values = c("Train Error" = "blue", "Test Error" = "red")) +
  scale_fill_manual(values = c("Train Error" = "blue", "Test Error" = "red")) +
  labs(x = "log10(lambda)", y = "Error", title = "Mean Train and Test Errors with Confidence Bands") +
  theme_minimal() 
```

A graph is plotted showing the mean squared error for both the test and train data vs number of covariate dimensions. The test MSE is represented by a red line and the train MSE is represented by a blue line. The train confidence interval is shown by a blue region and the test confidence interval is highlighted with a red region.
The train MSE increases gradually from 0 as the regulation parameter increases. The test error decreases initially and reaches a minimum at log10(lambda) = 1. From this point onwards the Test MSE increases slightly as the regularization parameter increases. However after a point (log10(lambda) = 3)) both the train and the test error have almost the same value. The train and the test error increase as the bias squared increases as the regulation parameter increases while the variance tends to 0. Thus after a point (in this case log10(lambda) = 3) the mean squared error is just the bias squared. This is seen in the next plot. 



There is another plot where the bias(squared) and variance are on the y-axis and the regularisation parameter is on the X Axis.
```{r}
# Create a data frame with lambda and bias/variance
bias_variance_data <- data.frame(lambda = lambda_seq, BiasSquared = bias_squared, Variance = variance)

# Plot the bias squared and variance
ggplot(bias_variance_data, aes(x = log10(lambda))) +
  geom_line(aes(y = BiasSquared, color = "Bias Squared")) +
  geom_line(aes(y = Variance, color = "Variance")) +
  scale_color_manual(values = c("Bias Squared" = "blue", "Variance" = "red")) +
  labs(x = "log10(lambda)", y = "Bias Squared / Variance") +
  theme_minimal()
```

In the graph the bias squared is represented by a line and the variance is represented by the red line. As we can see from the graph the bias squared and the variance behave in the opposite manner. The Mean Squared Error is a sum of the bias squared and the variance. As the regularization parameter increases the bias increases slightly and the variance decreases. The variance tends to 0 at log10(lambda) = 3. This happens because as the model becomes more restricted and simpler, sacrificing some flexibility to reduce the variance and improve generalization, the bias increases. Similarly this causes the variance to reduce before tending to 0 as the regularization parameter increases. Thus, the Mean Squared Error is caused solely by the bias squared.

There is also a plot for the coefficients of the Ridge Model vs the regularization parameter (lambda)
```{r}
# Create a data frame with lambda and coefficients
coefficients_data <- data.frame(lambda = lambda_seq, coefficients)

# Melt the data frame to long format for plotting
melted_coefficients <- reshape2::melt(coefficients_data, id.vars = "lambda")

# Plot the coefficients as a function of the hyperparameter
ggplot(melted_coefficients, aes(x = log10(lambda), y = value, color = variable)) +
  geom_line() +
  labs(x = "log10(lambda)", y = "Coefficient Value", title = "Coefficients as a Function of Hyperparameter") +
  theme_minimal() +
theme(legend.position = "none")

```

The coefficients for the Ridge Model stay constant for the initial values of lambda before converging to 0 at log10(lambda) = 3 (approx). This is similar to the variance tending to 0 too at the similar value of lambda. For the case of the regularization parameter lambda, the ridge estimates are just a scaled version of the least squares estimates. Ridge regression focuses on compressing the coefficients to become smaller. 